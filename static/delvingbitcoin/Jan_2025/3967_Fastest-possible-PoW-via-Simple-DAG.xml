<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Fastest-possible PoW via Simple DAG</title>
  <updated>2025-01-05T02:25:20.261206+00:00</updated>
  <author>
    <name>zawy 2025-01-04 23:43:19.353000+00:00</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Fastest-possible PoW via Simple DAG</title>
    <updated>2025-01-05T02:25:20.261238+00:00</updated>
    <link href="https://delvingbitcoin.org/t/fastest-possible-pow-via-simple-dag/1331/18" rel="alternate"/>
    <summary>The email delves into the intricacies of adjusting difficulty within a consensus system and specifically explores its implications for Braidpool, a concept yet to be fully conceptualized by the author. It begins by examining the expected orphan rate in a global Braidpool scenario, where with a 10% hash rate and a difficulty set at 1/1000th of standard blocks, an orphan rate of about 4% is anticipated. However, adjusting the target to aim for a 44% orphan rate suggests that beads (the smaller units within Braidpool) would need to be significantly easier to produce than initially estimated, roughly 14,500 times easier compared to standard blocks.

The discussion transitions into the mathematical foundation underlying the proposed difficulty adjustment algorithm (DAA), denoted as Nb/Nc, which aims to balance solvetimes against mean latency, revealing that this balance remains relatively unaffected by the hash rate but is sensitive to the distribution of latency across the network. This leads to an exploration of how different latency ratios among subsets of hashrate could influence the ability of the system to reach consensus, suggesting that extreme disparities in latency might hinder the integration of new data into the blockchain, while more moderate differences could be manageable.

Further, the email critiques a suggestion from another developer, Zawy, regarding the treatment of high-latency transactions, proposing instead that such transactions should not be excluded or penalized through non-payment but should be assessed through additional timing metrics to determine their validity without incentivizing undesirable mining behavior. The writer underscores the need for a nuanced approach to handling naturally occurring large cohorts and excessive "grandparents" within the blockchain, which are statistically expected due to the Poisson distribution of block arrivals, cautioning against overly simplistic adjustments to the difficulty algorithm that could inadvertently bias the system.

Lastly, it introduces two methods for mitigating the impact of high-latency transactions: the "great grandparent method," which assesses the appropriateness of including a block based on its relative position within the chain, and a dual approach combining time limit checks with the great grandparent rule to minimize false positives associated with either excessively slow or fast block confirmations. The email ultimately advocates for a balanced and statistically informed approach to difficulty adjustment in decentralized networks, emphasizing the importance of accommodating a range of latency experiences without compromising the integrity or efficiency of the system.</summary>
    <published>2025-01-04T23:43:19.353000+00:00</published>
  </entry>
</feed>
