<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>How to linearize your cluster</title>
  <updated>2025-02-02T02:18:46.068721+00:00</updated>
  <author>
    <name>sipa.384000+00:00</name>
  </author>
  <timestamp>2025-02-01T18:06:05.384000+00:00</timestamp>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>How to linearize your cluster</title>
    <updated>2025-02-02T02:18:46.068751+00:00</updated>
    <link href="https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303/16" rel="alternate"/>
    <summary>The discussion revolves around the concept of reusing a computational state for further analysis after the removal of the highest feerate closure to discover subsequent closures. The initial concern raised is about the complexity that might arise due to changes in the graph structure following the removal of a subset. However, this concern is addressed with an explanation that the graph's fundamental structure doesn't necessarily change. This is clarified by suggesting that the act of removing a previously found subset can be conceptualized as adjusting the fee and size of all its transactions to zero. This approach maintains the monotonicity of capacities requirement, which is crucial for the integrity of the graph-based analysis. Essentially, while a transaction remains, its capacity is determined by the formula $f-\lambda s$, but once it is considered removed or extracted from the analysis, its capacity effectively becomes zero. This methodology suggests a potentially efficient way to iterate over the data without reconstructing the graph for each iteration, thereby possibly enhancing the efficiency of finding successive high feerate closures within the same framework.</summary>
    <published>2025-02-01T18:06:05.384000+00:00</published>
  </entry>
</feed>
